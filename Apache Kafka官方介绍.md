# Apache Kafka官方介绍
原文链接：[Apache Kafka - INTRODUCTION](https://kafka.apache.org/intro)
## 什么是事件流处理（event streaming）？
事件流处理是人类中枢神经系统的数字形式的等效物。在当下这个永远在线（always-on）的世界，业务越来越由软件定义、越来越自动化，软件的使用者也越来越“软件”。事件流处理就是这种世界的基础。

从技术层面来说，事件流处理指的是如下行为：实时地将数据从事件源（如数据库、传感器、移动设备、云服务、软件应用程序等）以事件流的形式捕捉；持久地存储这些事件流以备取用；对事件流进行实时或延时（retrospectively）的操作、处理和反应；按需将事件流导向不同的目的地。因此，事件流处理保证了对数据的持续流动和处理，使得正确的信息可以在正确的时间处于正确的地点。
## 我可以用事件流处理做什么？
在许多企业和组织中，事件流处理都有着广泛的用途，包括：
- 实时处理支付和财政事务，例如在股票交易、银行、保险行业。
- 实时追踪和监控汽车、卡车、车队和货运，例如在物流和汽车工业。
- 持续捕捉和分析来自物联网设备等来源的传感器数据，例如在工厂和风电场。
- 收集和即时处理用户的互动和订单，例如在零售业、酒旅业和移动应用程序中。
- 监测住院病人并预测情况变化，确保紧急情况下的及时治疗。
- 连接、存储、提供公司中不同部门产生的数据。
- 作为数据平台、事务驱动架构和微服务的基础。
## Apache Kafka是一个事件流处理平台。这是什么意思？
Kafka结合了如下三个关键特性，因此您可以单用一个久经考验的解决方案来实现端到端事件流处理用例：
1. **发布**（publish）（写）和**订阅**（subscribe to）（读）事件流，包括持续地从其他系统导入/向其他系统导出数据。
2. 按照您的需求，长期、可靠地**存储**事件流。
3. 即时或延时地**处理**事件流。
而且，所有这些功能都是分布式的、高可扩展的、弹性的、容错的、安全的。Kafka可以被部署在裸机硬件、虚拟机和容器上，可以部署到本地或云端。您既可以自行管理Kafka环境，也可以使用来自众多供应商的完备的管理服务。
## 一句话，Kafka怎么工作？
Kafka是一个分布式系统，由**服务器**（servers）和**客户端**（clients）组成，它们使用一种高性能的TCP网络协议来通信。它可以被部署到本地或云端的裸机硬件、虚拟机或容器上。

**服务器**：Kafka是作为一个或多个服务器组成的集群来运行的，可以跨越多个数据中心或云区。这些服务器中，有一部分组成了存储层，它们被称为代理（brokers）。其他服务器上运行Kafka Connect，持续地将数据以事件流的形式导入或导出，将Kafka和您已有的系统（如关系数据库或其他Kafka集群）整合起来。为了让您完成关键的任务，Kafka集群具有高可扩展和容错的特点：如果集群中的任何服务器损坏，其他服务器会接管它的工作，保证持续的操作，并且不会丢失任何数据。

**客户端**：Kafka客户端允许您编写分布式的应用程序和微服务，它们可以并行、大规模、容错地读取、写入、处理事件流，即使有网络问题或者机器故障。Kafka中有一些这种客户端，它们又由Kafka社区提供的几十个客户端增强：客户端可用于Java和Scala，包括高级的Kafka Streams库，可用于Go、Python、C/C++和很多其他编程语言，以及REST API。

*（？？？）clients are available for Java and Scala including the higher-level Kafka Streams library, for Go, Python, C/C++, and many other programming languages as well as REST APIs*
## 主要概念和术语
一个**事件**（event）记录一个事实：在世界上或者你的业务中有事情发生。在文档中，它也被称为记录（record）或信息（message）。当你在Kafka中读或写数据时，你就是以事件的格式做这件事。概念上讲，一个事件包括一个键、一个值、一个时间戳，以及可选的元数据头。下面是一个事件样例：
```
Event key: "Alice"
Event value: "Made a payment of $200 to Bob"
Event timestamp: "Jun. 25, 2020 at 2:06 p.m."
```
**生产者**指的是向Kafka发布（写入）事件的客户端，而**消费者**指的是订阅（读取和处理）这些事件的客户端。在Kafka中，生产者和消费者完全解耦合，不知道彼此的存在，这是Kafka实现广为人知的高可扩展性的关键。例如，生产者完全不需要等待消费者。Kafka提供了多种多样的保证，例如处理事件恰好一次。

事件在**主题**（topic）里组织和持久存储。简单地说，主题就像是文件系统里的文件夹，而事件就是文件夹里的文件。例如，可以给一个主题命名为“支付”。Kafka里的主题总是多生产者、多消费者的：一个主题可以有0、1或更多生产者向其写入事件，也可以用0、1或更多消费者从中订阅事件。一个主题里的事件可以按需随时读取——不像传统的消息系统，事件在消费后不会被删除。相反地，您为每个主题设置Kafka要把事件保存多长时间，超时的事件会被抛弃。Kafka的性能与数据规模基本上无关，所以长期存储数据完全没有问题。
主题是**分区的**（partitioned），意思是一个主题分布在不同Kafka代理里的多个“桶”（bucket）里。这种分布式放置对于可扩展性非常重要，因为它使得客户端应用程序能够同时从/向多个代理读取/写入数据。当一个新事件发布到一个主题时，其实它是被添加到该主题的分区之一里。具有相同事件键（如一个用户或者交通工具标识符）的事件会被写入同一个分区，并且Kafka保证一个主题分区的任何消费者都一定会按照事件写入的顺序来读取事件。
![](/resource/streams-and-tables-p1_p4.png)
图：这个样例主题有4个分区P1-P4。两个不同的生产者客户端互相独立地通过网络向主题的分区里写入事件，以此向主题中发布新的事件。有着相同键（图中以相同颜色表示）的事件被写入相同分区。注意到在合适的情况下，不同的生产者可以写入同一个分区。

为了保证您的数据容错、高可用，每个主题都可以被**复制**（replicated），这甚至可以跨越地理区域或者数据中心，使得万一数据出错或者您想要维护代理，此时总是有多个代理保存着数据的复制。通常设置复制因子为3，也就是说，您的数据总是有3份复制。复制是在主题分区级别进行的。
本文作为介绍应该是足够的。如果您感兴趣，文档的设计（Design）部分详尽地解释了Kafka的各种概念。
## Kafka API
除了管理任务所用的命令行工具，Kafka为Java和Scala准备了5个核心API：
- 管理（Admin）API，用于管理和检查主题、代理和其他Kafka对象。
- 生产者API，用于向一个或多个Kafka主题发布（写入）一个事件流。
- 消费者API，用于订阅（读取）一个或多个主题，并处理生产到它们的事件流。
- Kafka流API，用于实现流处理应用程序和微服务。它提供一些处理事件流的高级功能，包括转化、有状态操作（如聚合和连接）、窗口化、基于时间的处理等。输入从一个或多个主题读取，输出到一个或多个话题，高效地将输入流转化为输出流。
- Kafka连接API，用于构建和运行可复用的数据输入、输出连接器，这些连接器从/向外部系统和应用程序消费（读取）/生产（写入）事件流，以此将它们和Kafka整合。例如，一个连接到关系数据库（例如PostgreSQL）的连接器可能会捕捉一些数据表的任何变动。但是，在实践中，您一般不需要实现自己的连接器，因为Kafka社区以及提供了成百上千现成的连接器。
## 从这里去哪里
想要获得Kafka的实践经验，请跟随*快速入门*。

想要更深入地理解Kafka，请阅读*文档*。您也可以选择Kafka书籍和学术论文。

浏览*样例*，了解全球社区里的其他用户是怎样利用Kafka的。

加入一个本地的Kafka小组，观看Kafka峰会的演讲。Kafka峰会是Kafka社区的主要会议。